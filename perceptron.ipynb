{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "perceptron.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNg9FBT9cVLN4K0K7h3N/53",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Raviezz/google-colab-pytorch/blob/master/perceptron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psBSKpoY5-uF",
        "colab_type": "code",
        "outputId": "da2f5704-a162-46fb-dd01-3d0d93b64617",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class NeuralNetwork():\n",
        "    \n",
        "    def __init__(self):\n",
        "        # Seed the random number generator\n",
        "        np.random.seed(1)\n",
        "\n",
        "        # Set synaptic weights to a 3x1 matrix,as my training set is 3x1\n",
        "        # with values from -1 to 1 and mean 0\n",
        "        self.synaptic_weights = 2 * np.random.random((3, 1)) - 1\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        \"\"\"\n",
        "        Takes in weighted sum of the inputs and normalizes\n",
        "        them through between 0 and 1 through a sigmoid function\n",
        "        Math behind it is , its approaching '1' as x approaches (+infinite) and its approaching '0' as x approaches (-infinite)\n",
        "        https://en.wikipedia.org/wiki/Logistic_function\n",
        "        \"\"\"\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sigmoid_derivative(self, x):\n",
        "        \"\"\"\n",
        "        The derivative of the sigmoid function used to\n",
        "        calculate necessary weight adjustments\n",
        "        This is adjust the synaptic weights , If we iteratively reduce each weight’s error, \n",
        "        eventually we’ll have a series of weights that produce good predictions.\n",
        "        \"\"\"\n",
        "        return x * (1 - x)\n",
        "\n",
        "    def train(self, training_inputs, training_outputs, epochs):\n",
        "        \"\"\"\n",
        "        We train the model through trial and error, adjusting the\n",
        "        synaptic weights each time to get a better result\n",
        "        \"\"\"\n",
        "        for epoch in range(epochs):\n",
        "            # Pass training set through the neural network\n",
        "            output = self.neuron(training_inputs)\n",
        "\n",
        "            # Calculate the error rate\n",
        "            error = training_outputs - output\n",
        "\n",
        "            # Multiply error by input and gradient of the sigmoid function\n",
        "            # Less confident weights are adjusted more through the nature of the function\n",
        "            adjustments = np.dot(training_inputs.T, error * self.sigmoid_derivative(output))\n",
        "            \"\"\"\n",
        "              training_inputs.T -> Transpose\n",
        "              np.dot is the dot product of the 3x1 matrices\n",
        "            \"\"\"\n",
        "\n",
        "            # Adjust synaptic weights\n",
        "            self.synaptic_weights += adjustments\n",
        "\n",
        "    def neuron(self, inputs):\n",
        "        \"\"\"\n",
        "        Pass inputs through the neural network to get output\n",
        "        \"\"\"  \n",
        "        inputs = inputs.astype(float)\n",
        "        output = self.sigmoid(np.dot(inputs, self.synaptic_weights))\n",
        "        return output\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Initialize the single neuron neural network\n",
        "    neural_network = NeuralNetwork()\n",
        "\n",
        "    print(\"Random starting synaptic weights: \")\n",
        "    print(neural_network.synaptic_weights)\n",
        "\n",
        "    # The training set, with 4 examples consisting of 3\n",
        "    # input values and 1 output value\n",
        "    training_inputs = np.array([[0,0,1],\n",
        "                                [1,1,1],\n",
        "                                [1,0,1],\n",
        "                                [0,1,1]])\n",
        "\n",
        "    training_outputs = np.array([[0,1,1,0]]).T\n",
        "\n",
        "    # Training the neural network\n",
        "    neural_network.train(training_inputs, training_outputs, 10000)\n",
        "\n",
        "    print(\"Synaptic weights after training: \")\n",
        "    print(neural_network.synaptic_weights)\n",
        "\n",
        "    A = str(input(\"Input 1: \"))\n",
        "    B = str(input(\"Input 2: \"))\n",
        "    C = str(input(\"Input 3: \"))\n",
        "    \n",
        "    print(\"New situation: input data = \", A, B, C)\n",
        "    print(\"Output data: \")\n",
        "    print(neural_network.neuron(np.array([A, B, C])))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random starting synaptic weights: \n",
            "[[-0.16595599]\n",
            " [ 0.44064899]\n",
            " [-0.99977125]]\n",
            "Synaptic weights after training: \n",
            "[[ 9.67299303]\n",
            " [-0.2078435 ]\n",
            " [-4.62963669]]\n",
            "Input 1: 1\n",
            "Input 2: 1\n",
            "Input 3: 1\n",
            "New situation: input data =  1 1 1\n",
            "Output data: \n",
            "[0.99211997]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}